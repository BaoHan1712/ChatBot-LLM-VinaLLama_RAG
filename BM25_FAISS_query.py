import os
from langchain.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever


embeddings = GPT4AllEmbeddings(model_file="models/vinallama-7b-chat_q5_0.gguf")

def load_pdf_data():
    pdf_path = "data/working.pdf"
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=128,
        chunk_overlap=50,
        separators=["\n\n", "\n", ".", "!", "?", ";", ":", ",", " ", ""],
        length_function=len,
        add_start_index=True
    )
    all_splits = text_splitter.split_documents(docs)
    
    bm25_retriever = BM25Retriever.from_documents(all_splits)
    bm25_retriever.k = 5 

    vector_store = FAISS.from_documents(
        all_splits, 
        embeddings,
        distance_strategy="METRIC_INNER_PRODUCT"
    )
    vector_store.save_local("faiss_index")

    return bm25_retriever

def print_results(results, query):
    print(f"\nüîç Top 5 k·∫øt qu·∫£ ph√π h·ª£p nh·∫•t cho c√¢u h·ªèi: '{query}'")
    
    for i, doc in enumerate(results[:5], 1):
        print(f"\n{i}. {doc.page_content}")
        print("-" * 50)
        
    if not results:
        print("‚ùå Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p.")

def hybrid_search(query, bm25_retriever, vector_store):
    """
    Hybrid search s·ª≠ d·ª•ng c·∫£ BM25 v√† FAISS ƒë·ªÉ t√¨m ki·∫øm k·∫øt qu·∫£ t·ªët nh·∫•t.
    """
    # L·∫•y k·∫øt qu·∫£ t·ª´ c·∫£ hai ph∆∞∆°ng ph√°p
    bm25_docs = bm25_retriever.get_relevant_documents(query)[:5]  
    faiss_docs = vector_store.similarity_search(query, k=5)  
    
    # K·∫øt h·ª£p k·∫øt qu·∫£ theo t·ª∑ l·ªá 50-50
    seen_content = set()
    final_docs = []
    
    # Xen k·∫Ω k·∫øt qu·∫£ t·ª´ c·∫£ hai ngu·ªìn
    for i in range(max(len(bm25_docs), len(faiss_docs))):
        if i < len(bm25_docs):
            if bm25_docs[i].page_content not in seen_content:
                seen_content.add(bm25_docs[i].page_content)
                final_docs.append(bm25_docs[i])
                
        if i < len(faiss_docs):
            if faiss_docs[i].page_content not in seen_content:
                seen_content.add(faiss_docs[i].page_content)
                final_docs.append(faiss_docs[i])
    
    return final_docs[:5]

def preprocess_query(query):
    query = query.lower().strip()
    
    # Lo·∫°i b·ªè c√°c t·ª´ stop word ti·∫øng Vi·ªát ph·ªï bi·∫øn
    stop_words = {"l√†", "v√†", "c√°c", "c·ªßa", "c√≥", "nh·ªØng", "ƒë·ªÉ", "trong", "cho"}
    query_words = [word for word in query.split() if word not in stop_words]
    
    # T·∫°o c√°c bi·∫øn th·ªÉ c·ªßa query
    variations = [
        " ".join(query_words),  # Query g·ªëc ƒë√£ lo·∫°i stop words
        " ".join(query_words[:3]),  # 3 t·ª´ ƒë·∫ßu ti√™n
        " ".join(query_words[-3:])  # 3 t·ª´ cu·ªëi c√πng
    ]
    
    return variations

def enhanced_test_search(bm25_retriever, vector_store):
    while True:
        query = input("\nNh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n (ho·∫∑c 'exit' ƒë·ªÉ tho√°t): ").strip()
        if query.lower() == "exit":
            print("üëã T·∫°m bi·ªát!")
            break
            
        query_variations = preprocess_query(query)
        
        all_results = []
        for q in query_variations:
            results = hybrid_search(q, bm25_retriever, vector_store)
            all_results.extend(results)
        
        unique_results = list({doc.page_content: doc for doc in all_results}.values())
        print_results(unique_results[:5], query)

if __name__ == "__main__":
    if not os.path.exists("faiss_index"):
        bm25_retriever = load_pdf_data()
    else:
        pdf_path = "data/working.pdf"
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=128,
            chunk_overlap=50,
            separators=["\n\n", "\n", ".", "!", "?", ";", ":", ",", " ", ""],
            length_function=len,
            add_start_index=True
        )
        all_splits = text_splitter.split_documents(docs)
        bm25_retriever = BM25Retriever.from_documents(all_splits)
        bm25_retriever.k = 5

    vector_store = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

    enhanced_test_search(bm25_retriever, vector_store)
